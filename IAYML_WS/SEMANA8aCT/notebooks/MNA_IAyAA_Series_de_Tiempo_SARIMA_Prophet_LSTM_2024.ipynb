{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aQ7eae1kIHDh",
        "W2Suo0VPaAu5",
        "vz2em5V6Nt3S",
        "uNgfKzCDmXHM",
        "JQ3Ri-mVtgaK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## **Inteligencia Artificial y Aprendizaje Automático**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Modelos sobre datos de Series de Tiempo: Ingenuo / SARIMA / Prophet / LSTM**\n"
      ],
      "metadata": {
        "id": "PGFd_MEwXGpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Los diferentes modelos que aplicamos en este ejercicio son utilizando la base de datos que nos muestran el total de pasajeros mensuales que viajaron en una aerolínea de EEUU de 1949 a 1960.**\n",
        "\n",
        "#### **La liga de los datos la puedes encontrar en la siguiente liga de Kaggle:**\n",
        "\n",
        "https://www.kaggle.com/datasets/chirag19/air-passengers\n",
        "\n"
      ],
      "metadata": {
        "id": "WPF5x85mhKPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0 - Preparando los datos**"
      ],
      "metadata": {
        "id": "aQ7eae1kIHDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Omitamos los Warnings por el momento:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "FVfKKUOGuUaI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instalar la librería de Kaggle\n",
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 2. Configurar las variables de entorno para la API de Kaggle\n",
        "# Colab jala automáticamente los 'secrets' que guardaste\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "print(\"¡Configuración de Kaggle lista!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8jaaug28cEN",
        "outputId": "49858318-293d-4d65-a6ef-84b909cfbd8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "¡Configuración de Kaggle lista!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "FVfKKNOGuUaI",
        "outputId": "d0a7f24e-b083-46a4-f928-dd27fa1baefd"
      },
      "source": [
        "# Downgrade numpy for compatibility with Prophet\n",
        "!pip uninstall numpy -y\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "# *** IMPORTANT: After running this cell, please restart the runtime (Runtime -> Restart runtime...). ***\n",
        "# After restarting, run the import cell (`1JLlOM-c5bhx`) again."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, which is not installed.\n",
            "spacy 3.8.7 requires numpy>=1.19.0; python_version >= \"3.9\", which is not installed.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, which is not installed.\n",
            "cufflinks 0.17.3 requires numpy>=1.9.2, which is not installed.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "torchvision 0.23.0+cu126 requires numpy, which is not installed.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, which is not installed.\n",
            "chex 0.1.90 requires numpy>=1.24.1, which is not installed.\n",
            "pandas-gbq 0.30.0 requires numpy>=1.18.1, which is not installed.\n",
            "optax 0.2.6 requires numpy>=1.18.0, which is not installed.\n",
            "osqp 1.0.5 requires numpy>=1.7, which is not installed.\n",
            "tensorflow-decision-forests 1.12.0 requires numpy, which is not installed.\n",
            "pytensor 2.35.1 requires numpy>=2.0, which is not installed.\n",
            "spanner-graph-notebook 1.1.8 requires numpy, which is not installed.\n",
            "tensorboard 2.19.0 requires numpy>=1.12.0, which is not installed.\n",
            "bigframes 2.28.0 requires numpy>=1.24.0, which is not installed.\n",
            "bqplot 0.12.45 requires numpy>=1.10.4, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "64291d7f0838434a9e82d70fa7bd079c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1JLlOM-c5bhx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "339b70cc-6f1b-4b74-d01e-63a39d608552"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "`np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1886415349.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprophet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsaplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonth_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquarter_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasonal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseasonal_decompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/prophet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree. An additional grant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# of patent rights can be found in the PATENTS file in the same directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprophet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecaster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/prophet/forecaster.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mNANOSECONDS_TO_SECONDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mProphet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \"\"\"Prophet forecaster.\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/prophet/forecaster.py\u001b[0m in \u001b[0;36mProphet\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mperiod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mseries_order\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     ) -> NDArray[np.float_]:\n\u001b[0m\u001b[1;32m    460\u001b[0m         \"\"\"Provides Fourier series components with the specified frequency\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "from prophet import Prophet\n",
        "from statsmodels.graphics.tsaplots import month_plot, quarter_plot\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf   # gráficos de autocorrelación"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip freeze   # Si deseas verificar qué librerías y versiones ya tiene google-Colab preinstaladas,\n",
        "               # en particular \"prophet\" que usaremos en este ejercicio."
      ],
      "metadata": {
        "id": "P2FBEthV-cna"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import kaggle\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "kaggle.api.dataset_download_files('chirag19/air-passengers', path='./', unzip=True)\n",
        "\n",
        "path = 'AirPassengers.csv' # Corrected filename to match available file\n",
        "df = pd.read_csv(path, header=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2MsyxVVo7n54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "54bce190-cdf2-4cc2-b7e5-9f7bdba9c3ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/chirag19/air-passengers\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-175626159.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AirPassengers.csv'\u001b[0m \u001b[0;31m# Corrected filename to match available file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()   # Observamos que la fecha es por el momento una variable de tipo objeto o categórica."
      ],
      "metadata": {
        "id": "cN4ZgvsP_1hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renombremos de manera estándar las columnas de fechas y valores de la\n",
        "# serie de tiempo, donde además la fecha la trasnformamos a tipo datetime:\n",
        "\n",
        "df.columns = ['ds', 'y']\n",
        "df['ds']= pd.to_datetime(df['ds'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GHe-4KW48CEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "NPfzZoVIAM9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En particular datetime64[ns] nos ayuda a representar un rango de fechas muy amplio (del año 1678 al 2262) con precisión de nanosegundos."
      ],
      "metadata": {
        "id": "2hXgMIbMAs2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agreguemos los ajustes usuales en Series de tiempo:**"
      ],
      "metadata": {
        "id": "4CvveIF0N22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hagamos una copia del DataFrame df con índices temporales:\n",
        "df_temp = df.copy()\n",
        "df_temp.set_index('ds', inplace=True)\n",
        "df_temp"
      ],
      "metadata": {
        "id": "fIEF8-B_N81E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df_temp.shape)"
      ],
      "metadata": {
        "id": "l0AuAN87O1l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html"
      ],
      "metadata": {
        "id": "l4eKSwonP3bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustemos la frecuencia de los índices, para que el modelo tenga información del período en que\n",
        "# se encuentran los datos:\n",
        "\n",
        "df_temp.index  # Observamos que aquí todavía no se tiene información del argumento \"freq\""
      ],
      "metadata": {
        "id": "wZ2gl3REPHNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para las opciones de \"freq\" ver la tabla que viene en esta liga:\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
      ],
      "metadata": {
        "id": "SxsQ-GAeQylZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Especifiquemos entonces que nuestros datos están registrados con una frecuencia mensual\n",
        "# al inicio del mes (cada día 01 de cada mes):\n",
        "df_temp.index.freq = 'MS'  # MS-Month-start-frequency o M=ME-Month-end-frequency\n",
        "\n",
        "df_temp.index"
      ],
      "metadata": {
        "id": "yn-ODCVEPUw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Particionemos en entrenamiento y prueba como DataFrames de Pandas:\n",
        "\n",
        "train = df_temp.iloc[0:-12,:]\n",
        "\n",
        "test = df_temp.iloc[-12:,:]   # Seleccionamos los datos del último año para predicción de Test."
      ],
      "metadata": {
        "id": "gIGzd7s3BMe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "Mf4pMFvpFTR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico con la periodicidad mensual:\n",
        "train['y'].plot(title='Pasajeros mensuales', style='-b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v1CJt0BRUvQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De este gráfico podemos observar:\n",
        "\n",
        "1.    Que tenemos una tendencia creciente, con una estacionalidad anual muy marcada\n",
        "\n",
        "2.   Que el modelo multiplicativo de la serie de tiempo se adaptaría mejor a uno aditivo, ya que la amplitud de la variabilidad se observa que va en aumento.\n",
        "\n"
      ],
      "metadata": {
        "id": "MtFVcjfFDYhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Modelo ingenuo (Baseline / Persistance / Naive model)**"
      ],
      "metadata": {
        "id": "W2Suo0VPaAu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   El modelo ingenuo simplemente hace la predicción del siguiente perídodo, repitiendo el valor del último período.\n",
        "\n",
        "*   Este modelo se utiliza en general como punto de partida para comparar todos los demás,\n",
        "\n",
        "*   Utilicemos el error RMSE (Raíz del Error Cuadrático Medio) como medida del desempeño de nuestros modelos a obtener."
      ],
      "metadata": {
        "id": "GoQObb3SnCxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test['y'].values"
      ],
      "metadata": {
        "id": "QPRdS74ULU-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [x for x in train['y'].values]   # Definimos \"history\" donde de manera incremental se irán agregando registros mensuales.\n",
        "predictions = list()    # Lista que contendrá las predicciones del modelo ingenuo.\n",
        "\n",
        "for i in range(len(test['y'])):\n",
        "  # Obtenemos las predicciones:\n",
        "  yhat = history[-1]   # La primera vez, tomamos el último dato del Train, en las siguientes, se van agregando los datos de Test.\n",
        "  predictions.append(yhat)\n",
        "\n",
        "  # Observaciones-registros reales:\n",
        "  obs = test.iloc[i]['y']   # Ahora vamos tomando los datos de Test.\n",
        "  history.append(obs)  # Vamos agregando a \"history\" (que ya tiene todo los de Train) en cada iteración, un dato más de los de Test.\n",
        "  print('>Esperado-real = %3d, Predicción = %.3f' % (obs, yhat))\n",
        "\n",
        "\n",
        "# Calculamos la raíz del error cuadrático medio (RMSE):\n",
        "rmse = np.sqrt(mean_squared_error(test['y'].values, predictions))\n",
        "print('\\nError-Modelo-Naive: RMSE: %.3f' % rmse)"
      ],
      "metadata": {
        "id": "NPYpnuIyaBK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Este valor de predicción RMSE nos dice que en promedio el error es de aproximadamente más menos 53 pasajeros por cada predicción hecha. Este será nuestro error RMSE a mejorar.**"
      ],
      "metadata": {
        "id": "FMYf6GU1teRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grafiquemos los resultados:\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax = plt.plot(train['y'], '-k')  # Datos de entrenamiento de 1949 a 1959 (los históricos).\n",
        "ax = plt.plot(test['y'], '+b')    # Datos de prueba reales del año 1960 (los del útimo año a predecir).\n",
        "ax = plt.plot(test.index, predictions, 'xr')  # Predicciones del modelo ingenuo para el último año 1960."
      ],
      "metadata": {
        "id": "6lnHihoIpSNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Aunque pareciera que las predicciones fueron bastante exactas en el gráfico, en realidad están todas desfasadas un año.**"
      ],
      "metadata": {
        "id": "F_3dwANunYgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Modelo ARIMA**"
      ],
      "metadata": {
        "id": "vz2em5V6Nt3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Análisis preliminar para obtener información de la serie de tiempo y construir el modelo ARIMA(p,d,q).**\n",
        "\n",
        "*   **AR (AutoRegresivo):** El modelo se basa en la relación entre un valor actual y sus valores pasados.\n",
        "\n",
        "*   **I (Integrado):** Se refiere a la diferenciación de la serie para hacerla estacionaria, es decir, eliminar tendencias y patrones cíclicos.\n",
        "\n",
        "*   **MA (Media Móvil):** Usa el error de los valores pasados (las desviaciones entre valores predichos y observados) para corregir el valor actual."
      ],
      "metadata": {
        "id": "TJfvrPN4N1M8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Ahora veamos el gráfico de descomposición de la serie de tiempo en sus componentes de tendencia (trend), estacionalidad (stationality) y residual (residual).**"
      ],
      "metadata": {
        "id": "UyXXfIULVBWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráficos de descomposición estacional:\n",
        "decomposition = seasonal_decompose(train['y'].resample('M').mean(), model='multiplicative')  # <<<---- Observa que el modelo es multiplicativo\n",
        "fig = decomposition.plot()\n",
        "fig.set_size_inches(8,6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XhNdKlhfauRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observa del gráfico de Residuos que estos están alrededor del 1, a diferencia del aditivo que usualmente están alrededor del 0.**"
      ],
      "metadata": {
        "id": "pKRJ36H6EQYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Veamos ahora estos dos gráficos que nos hablan si tenemos algún tipo de estacionalidad (i.e., periodicidad). En particular, mensual o trimestral:**"
      ],
      "metadata": {
        "id": "qIZIVslhaS7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfica para detección de estacionalidad mensual:\n",
        "fig = month_plot(train['y'].resample('M').mean(), ylabel='Pasajeros')\n",
        "fig.set_size_inches(6,6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4g2iw0RFWuGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si queremos graficar y detectar estacionalidad trimestral:\n",
        "fig = quarter_plot(train['y'].resample('Q').mean(), ylabel='Pasajeros')\n",
        "fig.set_size_inches(6,6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W335xglIWuDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gráficos de Autocorrelación (ACF) y Autocorrelación Parcial (PACF)**"
      ],
      "metadata": {
        "id": "0FaPO1PbEifQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Los gráficos de Autocorrelación miden la correlación entre la serie de tiempo y una versión rezagada de sí misma.\n",
        "\n",
        "*   Este tipo de correlaciones de una serie de tiempo consigo misma son llamadas \"autocorrelaciones\" o \"correlaciones seriales\".\n",
        "\n",
        "*   En el eje horizontal se muestra el valor del desfazamiento (lag) y en el eje vertical el valor de correlación de la serie desfazada y la original.\n",
        "\n",
        "*   El cono sombreado indica intervalos de confianza de la correlación, con un 95% de confianza de manera predeterminada.\n",
        "\n",
        "*   Que existan autocorrelaciones significativas en estos gráficos nos habla de que la serie de tiempo no es aleatoria.\n",
        "\n",
        "*   En estos dos gráficos (llamados en inglés \"pollipop plots\") hay que determinar cuántos de estos lollipops están afuera del intervalo de confianza (sin contar el inicial) antes de que entre el siguiente a la zona celeste?\n",
        "\n",
        "*   **Gráfico ACF:** Muestra la correlación total entre la serie y sus retardos, incluyendo efectos directos e indirectos.\n",
        "\n",
        "*   **Gráfico PACF:** Muestra la correlación directa entre la serie y sus retardos, eliminando los efectos de los retardos intermedios."
      ],
      "metadata": {
        "id": "pM-GWaBEd1O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico de Autocorrelación (ACF: Auto-Correlation-Function)\n",
        "fig,ax = plt.subplots(figsize = (6,4))\n",
        "plot_acf(train['y'], lags=60, ax=ax)   # Eliges el número de unidades (meses) desfasadas. El número de \"lags\" es el número\n",
        "plt.show()                          # que define la partición en el eje horizontal. El área no sombreada nos habla de\n",
        "                                    # la cantidad de períodos de tiempo hacia atrás cuya información proporcionada es\n",
        "                                    # significativa para la predicción futura.\n",
        "                                    # También nos ayuda a detectar periodicidades, como en este caso que se observa a 13 meses, aprox."
      ],
      "metadata": {
        "id": "CJPN4Sm9auKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El gráfico inicia con desfase (lag) de 0, lo cual es la correlación de la serie de tiempo consigo misma, la cual es 1.\n",
        "\n",
        "Posteriormente se muestran las correlaciones de la serie de tiempo original, con la serie desfasada \"q\" lugares hacia atrás, es decir, de orden \"q\" en la componente MA de ARIMA(p,d,q).\n",
        "\n",
        "**El gráfico ACF nos dice en primera instancia que la componente MA de ARIMA(p,d,q) tiene un grado \"q\" de 13 (en la práctica se puede buscar de 0 a 13).**"
      ],
      "metadata": {
        "id": "eQb3Q15lGf7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico de Autocorrelación Parcial (PACF):\n",
        "fig, ax = plt.subplots(figsize = (6,4))\n",
        "plot_pacf(train['y'], lags = 60, ax = ax, method=\"ols\")   # ols: regression of time series on lags of it an on constant. predetrminado method=\"ywm\"\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_u_ej3QOdcnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PACF también inicia con la correlación de la serie de tiempo consigo misma, por ello el valor de 1.\n",
        "\n",
        "*   **De este gráfico podemos observar que la componente AR de ARIMA(p,d,q) tiene grado p=3 o tal vez 5 (en la práctica se puede probar de 0 a 5.)**"
      ],
      "metadata": {
        "id": "hLBg_49hG00e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Existe un buen número de librerías para el caso de pronósticos con los modelos ARIMA-SARIMA. Usaremos en este ejercicio la de Statsmodels, cuya documentación encuentras en la siguiente liga:\n",
        "\n",
        "https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html"
      ],
      "metadata": {
        "id": "MrNDpYoRkM-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA"
      ],
      "metadata": {
        "id": "Au-3Pr8ptgQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo primero es para quitar la estacionalidad y transformar un problema\n",
        "# de tipo no-estacionario (SARIMA) en uno estacionario (ARIMA):\n",
        "\n",
        "# Diferencia de datos consecutivos en el tiempo\n",
        "# y podemos ahora usar ARIMA:\n",
        "def difference(dataset, interval=1):\n",
        "  diff = list()\n",
        "\n",
        "  for i in range(interval, len(dataset)):\n",
        "    value = dataset[i] - dataset[i - interval]\n",
        "    diff.append(value)\n",
        "\n",
        "  return np.array(diff)\n",
        "\n",
        "\n",
        "\n",
        "# Invierte las diferencias anteriores, es decir, nos regresa la estacionalidad:\n",
        "def inverse_difference(history, yhat, interval=1):\n",
        "  return yhat + history[-interval]\n"
      ],
      "metadata": {
        "id": "bCUPFTleP0bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definamos la siguiente lista de registros de pasajeros en el tiempo:\n",
        "history = [x for x in train.loc[:,'y'].values]   # train-ndarray : columna solo con los registros de pasajeros \"y\".\n",
        "                               # history-list : lista con los datos del train.\n",
        "\n",
        "predictions = list()   # Para incluir las predicciones que vaya generando el modelo.\n",
        "\n",
        "for i in range(len(test['y'])):\n",
        "  #\n",
        "  months_in_year = 12\n",
        "  diff = difference(history, months_in_year)    # diff-list: lista de los datos de diferencias para transformar SARIMA en ARIMA.\n",
        "\n",
        "  # Generamos el modelo.\n",
        "  model = ARIMA(diff, order=(3,0,13),) # Si aplicamos diff, entonces d=0 es aplicable.\n",
        "  model_fit = model.fit(method_kwargs={'maxiter':300})  # Aumentar las iteraciones para evitar el WarningConvergence.\n",
        "\n",
        "  yhat = model_fit.forecast()[0]   # prediccions (yhat) de cada registro, es un solo número flotante.\n",
        "  yhat = inverse_difference(history, yhat, months_in_year)    # El \"history\" sigue siendo la lista de arriba.\n",
        "                                                              # El nuevo \"yhat\" es cada un nuevo valor (float) de predicción.\n",
        "  predictions.append(yhat)  # predictions-list : lista que se va incrementando al ir agregando cada predicción nueva.\n",
        "\n",
        "  # Observaciones-reales:\n",
        "  obs = test.iloc[i]['y']   #  cada dato es un flotante.\n",
        "  history.append(obs)   #  history-list : se va incrementado a partir del Train con los registros-observaciones reales.\n",
        "\n",
        "  print('>Esperado-real = %3d, Predicción = %.3f' % (obs, yhat))\n",
        "\n",
        "# Error RMSE:\n",
        "rmse = np.sqrt(mean_squared_error(test['y'].values, predictions))\n",
        "print('\\nError-Modelo-ARIMA: RMSE: %.3f' % rmse)"
      ],
      "metadata": {
        "id": "lNcZ502WiOrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos su gráfico:\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax = plt.plot(train['y'], '-k')\n",
        "ax = plt.plot(test['y'], '-b')\n",
        "ax = plt.plot(test['y'], '+c')\n",
        "ax = plt.plot(test.index, predictions, 'xr')"
      ],
      "metadata": {
        "id": "4XwI-n2NlFDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Modelo Facebook-Meta-Prophet**"
      ],
      "metadata": {
        "id": "uNgfKzCDmXHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **En la liga del modelo Facebook-Prophet puedes encontrar mucho más información, además de la documentación correspondiente:**\n",
        "\n",
        "https://facebook.github.io/prophet/"
      ],
      "metadata": {
        "id": "tc9Ztuv3dttG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prophet requiere que la entrada sean dos columnas, llamadas \"ds\" y \"y\".\n",
        "# Les dmos esta forma usando el DataFrame \"df\" inicial:\n",
        "\n",
        "train = df.iloc[0:-12,:]\n",
        "test = df.iloc[-12:,:]"
      ],
      "metadata": {
        "id": "NvmPh6gp7CKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To fix the 'AttributeError: 'Prophet' object has no attribute 'stan_backend'' error,\n",
        "# we need to reinstall Prophet and its dependencies. This often resolves version conflicts.\n",
        "# UNINSTALL existing versions:\n",
        "!pip uninstall prophet pystan -y\n",
        "\n",
        "# INSTALL compatible versions:\n",
        "!pip install prophet==1.1.5 # Or a version known to be stable with current pystan\n",
        "!pip install pystan==2.19.1.1 --no-cache-dir # Specify a compatible pystan version\n",
        "# You might also need to install cmdstanpy if prophet uses it:\n",
        "# !pip install cmdstanpy\n",
        "\n",
        "# *** IMPORTANT: After running the above pip commands, please restart the runtime (Runtime -> Restart runtime...). ***\n",
        "# After restarting, run this cell again.\n",
        "\n",
        "# Aplicamos el modelo Facebbok-Prophet a nuestros datos:\n",
        "model = Prophet(seasonality_mode='multiplicative',\n",
        "                yearly_seasonality=True,\n",
        "                changepoint_prior_scale=10.,   # Controla la flexibilidad de la componente Trend, mayor valor mayores fluctuaciones.\n",
        "                seasonality_prior_scale=18)    # Controla la estacionalidad, mayor el valor, mayor flexibilidad en la estacionalidad.\n",
        "model.fit(train)"
      ],
      "metadata": {
        "id": "OoESktHC8FUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos el error RMSE:\n",
        "\n",
        "pred_FP = model.predict(test)['yhat']\n",
        "rmse = np.sqrt(mean_squared_error(test['y'], pred_FP))\n",
        "print('\\nError-Modelo-FacebookProphet: RMSE: %.3f' % rmse)"
      ],
      "metadata": {
        "id": "e6UjcfHdsSMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La librería misma nos permite obtener las predicciones con las cotas mínima\n",
        "# y máxima, además de su gráfica, como se muestra a continuación:\n",
        "\n",
        "forecast = model.predict(test)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "model.plot(forecast, ax=ax)               # Las predicciones se muestran con la línea continua, junto con su región de variabilidad de la predicción.\n",
        "ax = plt.plot(test['ds'], test['y'], '*r')   # Agregamos los datos de prueba (Test) con puntos rojos, para observar la diferencia.\n",
        "ax = plt.plot(test['ds'], forecast['yhat'], '+c')    # Valores reales.\n"
      ],
      "metadata": {
        "id": "Hq7eCZzU9iAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observamos que es algo conservador en sus predicciones, ya que se queda algo corto con las predicciones más altas y ligeramente por arriba de las predicciones más bajas.**"
      ],
      "metadata": {
        "id": "-pWto_Ct7d5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - Long Short Term Memory (LSTM)**"
      ],
      "metadata": {
        "id": "JQ3Ri-mVtgaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "LNWwO2e36RJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preparando_datos(myData, myDate):\n",
        "\n",
        "  # ++++++++ TRAIN +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "  # Los datos de entrenamiento son previos a la fecha \"Date\" que indiques y\n",
        "  # tomarán hisotriales en este caso de 12 períodos previos.\n",
        "  Train_Data = myData['y'][myData['ds'] < myDate].to_numpy()\n",
        "\n",
        "  periodo = 12   # 12 meses al año\n",
        "\n",
        "  Data_Train = []\n",
        "  Data_Train_X = []\n",
        "  Data_Train_Y = []\n",
        "\n",
        "  for i in range(0, len(Train_Data), periodo):    # observa que va formando el Train set con historiales de 12 períodos consecutivos.\n",
        "    try:\n",
        "      Data_Train.append(Train_Data[i : i + periodo])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "  # En ocasiones hay datos perdidos, en cuyo caso la última serie\n",
        "  # de datos la eliminamos:\n",
        "  if len(Data_Train[-1]) < periodo:\n",
        "    Data_Train.pop(-1)\n",
        "\n",
        "\n",
        "  # Hay que darle la forma (dimensionalidad) que requieren los modelos\n",
        "  # de aprendizaje profundo, en particular la librería de TensorFlow:\n",
        "  #     (batch_size, sequence_length, input_dimension).\n",
        "\n",
        "  Data_Train_X = Data_Train[0 : -1]\n",
        "  Data_Train_X = np.array(Data_Train_X)\n",
        "  Data_Train_X = Data_Train_X.reshape((-1, periodo, 1))\n",
        "\n",
        "  Data_Train_Y = Data_Train[1 : len(Data_Train)]\n",
        "  Data_Train_Y = np.array(Data_Train_Y)\n",
        "  Data_Train_Y = Data_Train_Y.reshape((-1, periodo, 1))\n",
        "\n",
        "\n",
        "  # +++++++++++ TEST +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "  # Los datos de prueba (Test) son posteriores a la fecha \"Date\" y que es el\n",
        "  # último ciclo que deseas predecir:\n",
        "  Test_Data = myData['y'][myData['ds'] >= myDate].to_numpy()\n",
        "\n",
        "  # inicialización de listas:\n",
        "  Data_Test = []\n",
        "  Data_Test_X = []\n",
        "  Data_Test_Y = []\n",
        "\n",
        "  for i in range(0, len(Test_Data), periodo):\n",
        "    try:\n",
        "      Data_Test.append(Test_Data[i : i + periodo])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "  if len(Data_Test[-1]) < periodo:\n",
        "    Data_Test.pop(-1)\n",
        "\n",
        "  Data_Test_X = Data_Test[0 : -1]\n",
        "  Data_Test_X = np.array(Data_Test_X)\n",
        "  Data_Test_X = Data_Test_X.reshape((-1, periodo, 1))\n",
        "\n",
        "  Data_Test_Y = Data_Test[1 : len(Data_Test)]\n",
        "  Data_Test_Y = np.array(Data_Test_Y)\n",
        "  Data_Test_Y = Data_Test_Y.reshape((-1, periodo, 1))\n",
        "\n",
        "\n",
        "  return Data_Train_X, Data_Train_Y, Data_Test_X, Data_Test_Y\n"
      ],
      "metadata": {
        "id": "Zo2CdfhX681G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuemos con el DataFrame como en Prophet:\n",
        "\n",
        "threshold_Date = '1959-01-01'   # A partir de esta fecha se empieza a generar el Test.\n",
        "                                # En este caso, el Text_X serán los 12 meses de 1959\n",
        "                                # y el Test_Y serán los de 1960.\n",
        "\n",
        "# Recuerda que la columna de fechas del DataFrame df, debe ser de tipo DateTime.\n",
        "# Se generan paquetes de 12 meses. Los *_Y son los años desfasados hacia adelante\n",
        "# de los *_X:\n",
        "\n",
        "Train_X, Train_Y, Test_X, Test_Y = preparando_datos(df, threshold_Date)"
      ],
      "metadata": {
        "id": "mh1c4-YfAWTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos definir la manera en que el tamaño de paso (learning-rate) varíe\n",
        "# en cada época. Es usual definir también algunas funciones, exponenciales,\n",
        "# por ejemplo. Por el momento definamos valores constantes por intervalos:\n",
        "\n",
        "def scheduler(epoch):\n",
        "\n",
        "  if epoch <= 100:\n",
        "    lrate = 0.05\n",
        "  elif epoch <= 180:\n",
        "    lrate = 0.0005\n",
        "  elif epoch <=190:\n",
        "    lrate = 0.00005\n",
        "  else:\n",
        "    lrate = (10 ** -5)\n",
        "\n",
        "  return lrate\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "metadata": {
        "id": "8q2bqddjtEfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myModel():\n",
        "\n",
        "  model = tf.keras.models.Sequential([tf.keras.layers.LSTM(20, input_shape=(12, 1), activation=tf.nn.leaky_relu, return_sequences=True),\n",
        "                                      tf.keras.layers.BatchNormalization(),\n",
        "                                      tf.keras.layers.Dropout(0.2),\n",
        "                                      #tf.keras.layers.LSTM(20, activation=tf.nn.leaky_relu),\n",
        "                                      #tf.keras.layers.BatchNormalization(),\n",
        "                                      #tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "                                      tf.keras.layers.Dense(200, activation=tf.nn.leaky_relu),\n",
        "                                      tf.keras.layers.Dropout(0.2),\n",
        "                                      #tf.keras.layers.Dense(120, activation=tf.nn.leaky_relu),\n",
        "\n",
        "\n",
        "                                      tf.keras.layers.Dense(12, activation=tf.nn.leaky_relu)\n",
        "                                      ])\n",
        "  return model\n",
        "\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "\n",
        "my_LSTM_model2 = myModel()\n",
        "\n",
        "my_LSTM_model2.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                      loss='mse',    # función de costo \"mse\"\n",
        "                      metrics=[tf.keras.metrics.RootMeanSquaredError()]   #  RMSE- despliega\n",
        "                      )\n",
        "\n",
        "\n",
        "H = my_LSTM_model2.fit(Train_X,\n",
        "                       Train_Y,\n",
        "                       epochs=200,\n",
        "                       validation_data=(Test_X, Test_Y),\n",
        "                       callbacks=[callback],\n",
        "\n",
        "                       )\n",
        "\n",
        "\n",
        "\n",
        "N = 50  # Para omitir los N primeros valores en la gráfica, que en ocasiones son\n",
        "        # demasiado grandes los errores y no dejan apreciar las curvas de aprendizaje.\n",
        "\n",
        "epochs = range(N+1, len(H.history[\"loss\"]) + 1)\n",
        "\n",
        "fig, ax2 = plt.subplots()\n",
        "fig.set_figheight(5)\n",
        "ax2.plot(epochs, H.history[\"root_mean_squared_error\"][N:], label = \"Training Root Mean Squared Error\")\n",
        "ax2.plot(epochs, H.history[\"val_root_mean_squared_error\"][N:], label = \"Validation Root Mean Squared Error\")\n",
        "ax2.set(xlabel = \"Epochs\", ylabel = \"Loss-RMSE\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tS3zdLmMSMR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos el error RMSE:\n",
        "\n",
        "#pred_LSTM = H.model.predict(Test_X)\n",
        "pred_LSTM = my_LSTM_model2.predict(Test_X).reshape(-1)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(test['y'].values, pred_LSTM))\n",
        "\n",
        "print('\\nError-Modelo-FacebookProphet: RMSE: %.3f' % rmse)"
      ],
      "metadata": {
        "id": "fe5YiSzhDD5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficamos los resultados:\n",
        "\n",
        "# Grafiquemos los resultados:\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax = plt.plot(train['ds'], train['y'], '-k')  # Datos de entrenamiento de 1949 a 1959.\n",
        "ax = plt.plot(test['ds'], test['y'], '-b')    # Datos de prueba del año 1960.\n",
        "ax = plt.plot(test['ds'], test['y'], '+c')    # Datos de prueba del año 1960.\n",
        "ax = plt.plot(test['ds'], pred_LSTM, 'xr')  # Predicciones del modelo del año 1960."
      ],
      "metadata": {
        "id": "qA7Ha0D4ERjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Habría que buscar la mejor arquitectura y ejecutarlo varias veces para obtener un mejor resultado.**"
      ],
      "metadata": {
        "id": "UKbYhHJjFiL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> ## **Fin del Ejercicio de Series de Tiempo**"
      ],
      "metadata": {
        "id": "18KiIqhsuoDr"
      }
    }
  ]
}